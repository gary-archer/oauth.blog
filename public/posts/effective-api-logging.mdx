# Effective API Logging

In the previous post I explained an <a href='api-journey-client-side.mdx'>API Journey - Client Side</a>, to describe technical behaviours OAuth clients may need to implement. This post explains a way to design API technical support logs to meet people requirements, so that they can run useful <a href='api-technical-support-analysis.mdx'>technical support queries</a>.

### Logging Frameworks

Most online documents on API logging describe a widely used development approach, with the following generic capabilities. These behaviours are typically provided by a logging framework:

- Enable a *logger per class*.
- Enable *logging levels*, such as DEBUG, INFO, WARN and ERROR.
- Output log data to various locations via *appenders* or *transports*.
- Enable logging behaviour to be changed via *configuration*.

Results can be useful for local development, with a single user and a low volume of API requests. Yet it may not scale effectively when you need to manage a large volume of logs.

### Common Logging Limitations

The following table summarises some common limitations you may run into with your API logs:

| Problem Area | Description |
| ------------ | ----------- |
| Difficult to Query | Logger per class output may use free text and as a result only support text find operations. |
| Does not Scale | Under load it may be difficult to answer basic questions such as how many errors of a particular type there have been today. |
| Not Configurable | In some setups it can be impossible to get stakeholders to change the production log level when there is a problem, for fear they will make conditions worse. |

### Logging Requirements

In such cases, starting with your requirements can work better than following a technology-first approach. This blog uses the following people requirements:

| Requirement | Behaviour |
| ----------- | --------- |
| Logs are Centralized | Technical support users go to a single known location to find API logs for all API instances. |
| Logs are Structured | Logs contain data that subgroups of people want to query, and each log entry can consist of both known and unknown fields. |
| Logs are Easy to Use | Any semi-technical person can issue basic queries against the log data, with a small learning curve. |
| Logs are Lightweight | Logging avoids redundant noise, so that logs are readable and do not impact performance. |
| Logs are Secure | Logs should not usually include confidential data, such as personal information or request bodies. |
| Logging is Always On | Incidents are always in the past and changing the log level after the event is too late. |
| Logging is the Same Everywhere | Logging works the same on a development computer as in production. |

You can capture multiple types of structured logs, for different types of people. In many cases a generic solution works well enough, rather than needing to implement custom logging logic per business area.

- Various teams may look at logs for technical support purposes.
- Security teams may want to look at API audit logs.
- Product owners may want to look at business activity logs.

The most important API logs are usually those used for fast problem resolution. The The primary goal of reliability logging is a system that is easy for the following people to use. It must be able to deal with busy production systems, with hundreds of concurrent users, or a million API calls per day.

| Role | Usage |
| ---- | ----- |
| Developer | Looks at logs while coding. |
| Tester | Looks at logs during various types of testing. |
| DevOps | Looks at production logs to investigate incidents. |

### Log Aggregation

This blog's final APIs write structured logs to standard output or a text file, which are fast operations. In modern API hosting, the platform creates files by redirecting *stdout* to a file. A log shipper component reads log files and sends the log data to a data store from where logs can be queried. Log aggregation is very standard these days, and this blog uses the free open source [Elastic Stack](https://www.elastic.co/elastic-stack), deployed using containers. The essential part of the solution is for API logs to contain good quality data.

### Multiple Logger Types

To build your preferred logging, you can still use a logging framework, but you may need to adapt its default behaviour. Frameworks use loggers to represent a type of output data.  In this blog's final APIs I use the following logs and I only aggregate request and audit logs:

| Log Type | Output |
| -------- | ------ |
| Request Log | Logs that contain a JSON object per API request for technical support purposes. |
| Audit Log | Logs that contain a JSON object per API request for security monitoring purposes. |
| Debug Logs | Logs that contain free text, which might be useful in single user development setups. |

In more complex APIs, the logger per class data at an INFO level may also be useful in production. If so, you can also aggregate those logs, then join it to other log types using a *Correlation ID*.

### Log Entries

For each API request I output a single request log entry and a single audit log entry, with the same ID fields. The output has similarities to an HTTP server log entry. However, you collect fields that are useful to query by instead of only HTTP-related values.

```json
{
  "type": "request",
  "id": "c4939e2c-9f71-4f4b-bbca-dda287b48385",
  "utcTime": "2022-07-24T08:41:05.069Z",
  "apiName": "FinalApi",
  "operationName": "getCompanyTransactions",
  "hostName": "UBUNTU",
  "method": "GET",
  "path": "/investments/companies/2/transactions",
  "resourceId": "2",
  "clientName": "LoadTest",
  "userId": "a6b404b1-98af-41a2-8e7f-e4061dc0bf86",
  "statusCode": 200,
  "millisecondsTaken": 7,
  "correlationId": "3e4ac756-11c7-e60f-c564-ad4f203d5742",
  "sessionId": "a601559a-0c90-c899-8099-8a9f63a30be8"
}
{
  "type": "audit",
  "id": "c4939e2c-9f71-4f4b-bbca-dda287b48385",
  "utcTime": "2022-07-24T08:41:05.069Z",
  "apiName": "FinalApi",
  "operationName": "getCompanyTransactions",
  "hostName": "UBUNTU",
  "method": "GET",
  "path": "/investments/companies/2/transactions",
  "resourceId": "2",
  "clientName": "LoadTest",
  "userId": "a6b404b1-98af-41a2-8e7f-e4061dc0bf86",
  "statusCode": 200,
  "millisecondsTaken": 7,
  "correlationId": "3e4ac756-11c7-e60f-c564-ad4f203d5742",
  "sessionId": "a601559a-0c90-c899-8099-8a9f63a30be8",
  "isAuthenticated": true,
  "isAuthorized": true,
  "scope": [
    "openid",
    "profile",
    "investments"
  ],
  "claims": {
    "managerId": "10345",
    "role": "user"
  }
}
```

### Log Schemas

This blog's API request logs use a schema with the following top level fields, many of which capture contextual information. Users investigating technical problems can filter logs by any of these fields:

| Field | Description |
| ----- | ----------- |
| Type | Set to *request* for technical support logs. |
| ID | A globally unique identifier for the log entry. |
| UTC Time | The time when the API code received the request. |
| API Name | The name of the API within the platform. |
| Operation Name | The name of the operation within the API. |
| Host Name | The name of the server that hosts this API instance. |
| HTTP Method | Whether a GET, POST etc. |
| Path | The URL path and query string. |
| Resource ID | The REST URL runtime path segments that identify the resource. |
| Client Name | A readable value for the application that called the API. |
| User ID | The subject claim from the OAuth access token. |
| Status Code | The response status code. |
| Milliseconds Taken | The number of milliseconds that the API code took to execute. |
| Error Code | When an error occurs this field contains a text code to identify the cause of the error. |
| Error ID | When an API 500 error occurs this field contains a generated number to track the exact error occurrence. |
| Correlation ID | An identifier either supplied via a request header or which the API generates. |
| Session ID | Used to partition multiple related calls to the API together, such as those for a frontend user session or load test. |

In addition, request logs include the following child objects which you cannot query directly. Instead, you use top level fields to get documents that contain these objects.

| Field | Description |
| ----- | ----------- |
| Performance | Provides instrumentation on expensive subtasks to help understand where time is spent. |
| Error | When the API returns a 400 or 500 response, this contains the client error response, along with context and exception details where applicable. |
| Info | Additional arbitrary data can be added here, though I use this object sparingly. |

For audit logs I include most of the contextual fields and add the following additional fields. I omit the more detailed technical support details from audit logs.

| Field | Description |
| ----- | ----------- |
| isAuthenticated | True if the API receives an access token that passes JWT validation checks. |
| isAuthorized | True if the API request is authenticated and the API does not return an unauthorized response. |
| scope | The scope values received in the access token. |
| claims | The main claims used for API authorization. |

### Logs and Sensitive Data

Some types of logs, like audit logs, may need to contain sensitive details about users. Therefore, only a particular audience, like a security team, should be able to view those logs. Keep sensitive data out of request logs so that you can grant engineers access to production request logs. I use an anonymous user identifier, like a UUID, as the OAuth subject claim and write that to all types of logs.

### API Logging Deployed Configuration

This blog's final APIs use Node.js, .NET and Java. Each API defines its logging behaviour in its configuration file. Deployed environments  the following log configuration, to write bare JSON to standard output.

```json
{
  "logging": {
    "apiName": "FinalApi",
    "loggers": [
      {
        "type": "request",
        "performanceThresholdMilliseconds": 500,
        "transports": [
          {
            "type": "console",
            "prettyPrint": false
          }
        ]
      },
      {
        "type": "audit",
        "transports": [
          {
            "type": "console",
            "prettyPrint": false
          }
        ]
      }
    ]
  }
}
```

Typically, the platform captures *stdout* and saves log output to files. Log shipper components read lines of bare JSON from the file and send them to a log aggregation system.

![bare logs](../images/280/bare-logs.jpg)

### API Logging Local Configurations

For local development, I give myself other choices. By default I only output request logs and use multiline JSON for best readability. If I want to use log aggregation I also output logs to local files. I can use debug logs if I set a log level of *debug* for one or more API classes.

```json
{
  "logging": {
    "apiName": "FinalApi",
    "loggers": [
      {
        "type": "request",
        "performanceThresholdMilliseconds": 500,
        "transports": [
          {
            "type": "console",
            "prettyPrint": true
          },
          {
            "type": "file",
            "filePrefix": "request",
            "dirname": "./logs",
            "maxSize": "10m",
            "maxFiles": "7d"
          }
        ]
      },
      {
        "type": "audit",
        "transports": [
          {
            "type": "file",
            "filePrefix": "audit",
            "dirname": "./logs",
            "maxSize": "10m",
            "maxFiles": "7d"
          }
        ]
      },
      {
        "type": "debug",
        "level": "info",
        "overrideLevels": {
          "ClaimsCache": "debug"
        }
      }
    ]
  }
}
```

I can then write code of the following form if I want to use logger per class output.

```typescript
const debugLogger = loggerFactory.getDevelopmentLogger(ClaimsCache.name);
debugLogger.debug(`Token to be cached will expire in 
                   ${secondsToCache} seconds (hash: ${accessTokenHash})`);
```

### Error Logs

When APIs return an HTTP 400 related status, request logs record the client response error along with additional context to explain the technical cause. The API code throws an exception, but no call stack is logged, since nothing has failed on the server:

```json
{
  "type": "request",
  "id": "7af62b06-8c04-41b0-c428-de332436d52a",
  "utcTime": "2022-07-24T10:27:33.468Z",
  "apiName": "FinalApi",
  "operationName": "getCompanyTransactions",
  "hostName": "UBUNTU",
  "method": "GET",
  "path": "/investments/companies/2/transactions",
  "resourceId": "2",
  "clientName": "FinalSPA",
  "statusCode": 401,
  "errorCode": "invalid_token",
  "millisecondsTaken": 2,
  "correlationId": "15b030a2-c67d-01ae-7c3f-237b9a70dbba",
  "sessionId": "77136323-ec8c-dce2-147a-bc52f34cb7cd",
  "errorData": {
    "statusCode": 401,
    "clientError": {
      "code": "invalid_token",
      "message": "Missing, invalid or expired access token"
    },
    "context": "JWT verification failed : signature verification failed"
  }
}
```

When API requests return an HTTP 500 related status, request logs include both the client and service errors. In this case, technical information, including a call stack, represent the problem cause:

```json
{
  "type": "request",
  "id": "b36701c9-ddf2-d7da-df48-4dfcc918009b",
  "utcTime": "2022-07-24T10:28:00.435Z",
  "apiName": "FinalApi",
  "operationName": "getCompanyTransactions",
  "hostName": "UBUNTU",
  "method": "GET",
  "path": "/investments/companies/2/transactions",
  "resourceId": "2",
  "clientName": "FinalSPA",
  "userId": "a6b404b1-98af-41a2-8e7f-e4061dc0bf86",
  "statusCode": 500,
  "errorCode": "exception_simulation",
  "errorId": 79072,
  "millisecondsTaken": 9,
  "correlationId": "5f1f1bcb-79c4-00ee-a1fe-be5e4262eb75",
  "sessionId": "77136323-ec8c-dce2-147a-bc52f34cb7cd",
  "errorData": {
    "statusCode": 500,
    "clientError": {
      "code": "exception_simulation",
      "message": "An unexpected exception occurred in the API",
      "id": 79072,
      "area": "FinalApi",
      "utcTime": "2022-07-24T10:28:00.438Z"
    },
    "serviceError": {
      "details": "",
      "stack": [
        "Error: An unexpected exception occurred in the API",
        "at Function.createServerError (/Users/gary/dev/oauth.apisample.nodejs/src/plumbing/errors/errorFactory.ts:16:16)",
        "at CustomHeaderMiddleware.processHeaders (/Users/gary/dev/oauth.apisample.nodejs/src/plumbing/middleware/customHeaderMiddleware.ts:27:36)",
        "at Layer.handle [as handle_request] (/Users/gary/dev/oauth.apisample.nodejs/node_modules/express/lib/router/layer.js:95:5)",
        "at trim_prefix (/Users/gary/dev/oauth.apisample.nodejs/node_modules/express/lib/router/index.js:328:13)",
        "at /Users/gary/dev/oauth.apisample.nodejs/node_modules/express/lib/router/index.js:286:9",
        "at param (/Users/gary/dev/oauth.apisample.nodejs/node_modules/express/lib/router/index.js:365:14)",
        "at param (/Users/gary/dev/oauth.apisample.nodejs/node_modules/express/lib/router/index.js:376:14)",
        "at Function.process_params (/Users/gary/dev/oauth.apisample.nodejs/node_modules/express/lib/router/index.js:421:3)",
        "at next (/Users/gary/dev/oauth.apisample.nodejs/node_modules/express/lib/router/index.js:280:10)",
        "at ClaimsCachingAuthorizer.authorizeRequestAndGetClaims (/Users/gary/dev/oauth.apisample.nodejs/src/plumbing/security/baseAuthorizer.ts:62:13)"
      ]
    }
  }
}
```

### Performance Instrumentation

In places the API code contains statements to measure the performance of a particular routine. The time taken may need to include one or more *async await* operations:

```typescript
public async getUserInfo(accessToken: string): Promise<UserInfoClaims> {

    using breakdown = this.logEntry.createPerformanceBreakdown('userInfoLookup');

    try {

        const response = await axios.request(options as AxiosRequestConfig);
        return ClaimsReader.userInfoClaims(response.data);

    } catch (e: any) {

        throw ErrorUtils.fromUserInfoError(e, this.configuration.userInfoEndpoint);
    }
}
```

To see this output in the final APIs, change the logging configuration to use a zero performance threshold. The request logs then contain a number of performance items to provide a breakdown of time spent:

```json
{
  "type": "request",
  "id": "baf31c4c-6bf3-5ba3-2863-169a088b4776",
  "utcTime": "2022-07-24T10:30:10.697Z",
  "apiName": "FinalApi",
  "operationName": "getCompanyList",
  "hostName": "UBUNTU",
  "method": "GET",
  "path": "/investments/companies",
  "clientName": "FinalSPA",
  "userId": "a6b404b1-98af-41a2-8e7f-e4061dc0bf86",
  "statusCode": 200,
  "millisecondsTaken": 383,
  "correlationId": "b4b1f41c-abcb-f99f-e8fd-0193ff7c2099",
  "sessionId": "77136323-ec8c-dce2-147a-bc52f34cb7cd",
  "performance": {
    "name": "total",
    "millisecondsTaken": 383,
    "children": [
      {
        "name": "validateToken",
        "millisecondsTaken": 84
      },
      {
        "name": "userInfoLookup",
        "millisecondsTaken": 292
      },
      {
        "name": "selectCompanyListData",
        "millisecondsTaken": 1
      }
    ]
  }
}
```

Each performance section also has a *details* field, which can store additional information. For example, you could extend logging code to record sanitized SQL and parameters but only output it when there is an error or performance exceeds the threshold.

### Client Context

API clients contribute useful fields to logs with the following three custom HTTP headers:

- A *Correlation ID* for the exact request.
- A *Session ID* for all requests in the same authenticated user session.
- A *Client Name* to easily identify the client.

This *Client Context* helps to satisfy <a href='api-technical-support-analysis.mdx'>query use cases</a>, including the ability to quickly filter logs for a frontend session ID. Calls to upstream APIs can forward the client context in request headers, so that the upstream API includes these identifiers in its own log entry.

![application session](../images/280/application-session.jpg?v=20240913)

### Logging Implementation

An implementation needs to populate a *LogEntry* object stored in-memory during the lifetime of the API request, then output its data when the request ends. This is trickier to code than when using a logger per class:

- You may need to capture identity details in an *OAuth Filter*.
- You may need to capture HTTP request details in a *Logging Filter*.
- You may need to capture exception details in an *Error Filter*.
- Business logic may sometimes need to use the log entry and contribute data to logs.

The log entry is a natural request scoped object, and you may need to use it in multiple classes. This log's final APIs use dependency injection to inject the request-scoped log entry into objects that need it. I also ensure that this works correctly when API code resumes on a different thread after async await requests complete.

### Non REST Operations

In other backend operations I follow the same approach. For example, if a *Billing API* subscribes to a backend event named *OrderCreated* and creates an invoice, that unit of work could log to the same schema, with similar coding techniques. I would write the following type of request log entry, as a logical REST operation rather that a physical one:

```json
{
  "type": "request",
  "id": "c4939e2c-9f71-4f4b-bbca-dda287b48385",
  "utcTime": "2022-07-24T08:41:05.069Z",
  "apiName": "FinalApi",
  "operationName": "OrderCreated",
  "hostName": "UBUNTU",
  "method": "POST",
  "path": "/invoices/777",
  "resourceId": "777",
  "clientName": "MessageBroker",
  "userId": "a6b404b1-98af-41a2-8e7f-e4061dc0bf86",
  "statusCode": 200,
  "millisecondsTaken": 7,
  "correlationId": "3e4ac756-11c7-e60f-c564-ad4f203d5742",
  "sessionId": "a601559a-0c90-c899-8099-8a9f63a30be8"
}
```

Although in this case there is no physical HTTP request, assigning failures a status code is a widely understood way to convey a high level result to people. Separating the client and service parts of an error also remains a good practice, in case you need to notify a recipient of failures without disclosing the full exception details.

### Tracing Standardization

I could update this blog's API logging to output performance details as [OpenTelemetry Traces](https://opentelemetry.io/). The *Correlation ID* in the log schema would then change to an OpenTelemetry *Trace ID*. I would record performance breakdowns as *spans* in the standardized trace format. Logs and traces are closely related, so you should be able to query both in the same observability system. Logging solutions like the Elastic Stack might enable this, but the ability to ask your desired questions of log data will always be the area of highest value.

### Where Are We?

I articulated how this blog's logging would behave in a platform of APIs. Next, I explain this blog's approach to handling errors and enabling fast problem resolution, both during and after development.

### Next

- I take a detailed look at <a href='error-handling-and-supportability.mdx'>Error Handling and Supportability</a>.
- For a list of all blog posts see the <a href='index.mdx'>Index Page</a>.
